{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Add the sigmoid layer in the forward pass before returning the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2952],\n",
      "        [-0.2107],\n",
      "        [-0.3156],\n",
      "        [-0.3131],\n",
      "        [-0.4433],\n",
      "        [-0.3957],\n",
      "        [-0.3959],\n",
      "        [-0.3432],\n",
      "        [-0.3959],\n",
      "        [-0.3420]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Output layer for binary classification (1 node)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Extract the last hidden state output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Pass the last hidden state output to fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # TODO: Apply sigmoid function to convert to a probability\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10   # Number of input features\n",
    "hidden_size = 20  # Number of features in the hidden state\n",
    "num_layers = 1    # Number of stacked RNN layers\n",
    "\n",
    "# Create the RNN model instance\n",
    "model = SimpleRNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Example input (batch_size, sequence_length, input_size)\n",
    "sequence_length = 100\n",
    "data_size = 10\n",
    "x = torch.rand(data_size, sequence_length, input_size)\n",
    "y = torch.randint(0, 2, (data_size, 1)).type(torch.FloatTensor)\n",
    "# Get the model output\n",
    "output = model(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Train the SimpleRNN using the Adam optimizer \n",
    "- Hint:  you can copy the code from pytorch_neural_network_solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dnjfd8ot) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_Adam_RNN</strong> at: <a href='https://wandb.ai/factiverse-ai/dat550/runs/dnjfd8ot' target=\"_blank\">https://wandb.ai/factiverse-ai/dat550/runs/dnjfd8ot</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240312_134350-dnjfd8ot/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dnjfd8ot). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vsetty/repos/dat550/2024/handson_rnns/wandb/run-20240312_134458-kwb4g8y5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/factiverse-ai/dat550/runs/kwb4g8y5' target=\"_blank\">run_Adam_RNN</a></strong> to <a href='https://wandb.ai/factiverse-ai/dat550' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/factiverse-ai/dat550' target=\"_blank\">https://wandb.ai/factiverse-ai/dat550</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/factiverse-ai/dat550/runs/kwb4g8y5' target=\"_blank\">https://wandb.ai/factiverse-ai/dat550/runs/kwb4g8y5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.696307897567749\n",
      "Epoch 10, Loss: 0.6790949106216431\n",
      "Epoch 20, Loss: 0.6604824066162109\n",
      "Epoch 30, Loss: 0.6382136344909668\n",
      "Epoch 40, Loss: 0.6076628565788269\n",
      "Epoch 50, Loss: 0.5811401605606079\n",
      "Epoch 60, Loss: 0.5506598949432373\n",
      "Epoch 70, Loss: 0.5198476314544678\n",
      "Epoch 80, Loss: 0.5441415905952454\n",
      "Epoch 90, Loss: 0.49525484442710876\n",
      "Final Loss: 0.46601733565330505\n"
     ]
    }
   ],
   "source": [
    "criterion = # TODO: Define the loss function\n",
    "optimizer = # TODO: Define the optimizer\n",
    "name = \"Adam_RNN\"\n",
    "# Training loop (for demonstration, let's say 100 epochs)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # TODO: Forward pass\n",
    "    # TODO: Compute loss\n",
    "    # TODO: Backward pass\n",
    "\n",
    "# Print final loss\n",
    "print(f'Final Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Change the SimpleRNN to use the GRU units instead of RNN units and trin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Implement a SimpleLSTM using the LSTM unit. Note you also need to include the c0 in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = # TODO: Define the LSTM layer\n",
    "        # Output layer for binary classification (1 node)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        # TODO: Get the output and hidden state from the lstm\n",
    "\n",
    "        # Extract the last hidden state output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Pass the last hidden state output to fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # Apply sigmoid function to convert to a probability\n",
    "        out = torch.sigmoid(out).squeeze()  # You may want to remove squeeze if expecting batch_size > 1\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Create the LSTM model instance\n",
    "model = SimpleLSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "\n",
    "# Get the model output\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Log the learning curve to wandb and repeat all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# Register an account in https://wandb.ai/ and get your API key from https://wandb.ai/authorize\n",
    "wandb.login(key=\"add your key here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
